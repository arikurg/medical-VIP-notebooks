{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“˜ Section 0 â€“ Machine Learning Foundations Refresher\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "In this section, you'll revisit the key concepts that underpin all modern machine learning systems, including NLP and LLMs. By the end, you should be able to:\n",
        "\n",
        "- Distinguish between supervised, unsupervised, and self-supervised learning.\n",
        "- Explain what optimization means in the context of ML.\n",
        "- Recall common evaluation metrics and when to use them.\n",
        "- Build and evaluate a simple classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ What Is Machine Learning?\n",
        "\n",
        "At its core, machine learning (ML) is about learning patterns from data to make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "An ML system typically consists of:\n",
        "\n",
        "- **Data**: examples from which patterns are learned.\n",
        "- **Model**: a mathematical function with parameters.\n",
        "- **Objective (Loss) function**: measures how far predictions are from reality.\n",
        "- **Optimization algorithm**: updates the model to minimize loss.\n",
        "- **Evaluation**: checks how well the model generalizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ Types of Machine Learning\n",
        "\n",
        "| Type | Description | Example Tasks | Example Algorithms |\n",
        "|------|-------------|---------------|--------------------|\n",
        "| **Supervised** | Learn from labeled data (input â†’ output pairs). | Classification, Regression | Logistic Regression, SVM, Neural Nets |\n",
        "| **Unsupervised** | Find structure in unlabeled data. | Clustering, Dimensionality Reduction | K-Means, PCA, Autoencoders |\n",
        "| **Self-Supervised** | Create labels from the data itself (predict masked/next parts). | Masked LM (BERT), Contrastive Learning | BERT, SimCLR |\n",
        "| **Reinforcement** | Learn by interacting with an environment and receiving rewards. | Game-playing, Robotics | Q-Learning, PPO |\n",
        "\n",
        "### ðŸ§  Intuition:\n",
        "\n",
        "- **Supervised** = teacher gives you answers.\n",
        "- **Unsupervised** = you group things yourself.\n",
        "- **Self-supervised** = you hide parts of data and try to predict them.\n",
        "- **Reinforcement** = you learn by trial and error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ Optimization: How Models Learn\n",
        "\n",
        "Training = finding model parameters Î¸ that minimize a loss function L(Î¸).\n",
        "\n",
        "$$\\theta^* = \\arg\\min_\\theta L(\\theta)$$\n",
        "\n",
        "### Common optimizers:\n",
        "\n",
        "**Stochastic Gradient Descent (SGD)**:\n",
        "\n",
        "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)$$\n",
        "\n",
        "**Adam**: an adaptive version that tracks momentum and variance.\n",
        "\n",
        "### ðŸ§  Analogy: \n",
        "Imagine descending a mountain (loss surface) in fog â€” the gradient tells you which way is downhill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Playground: Visualizing Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a simple convex function\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = x**2\n",
        "\n",
        "# Simulate gradient descent\n",
        "lr = 0.3\n",
        "theta = 4\n",
        "history = [theta]\n",
        "for _ in range(10):\n",
        "    grad = 2 * theta\n",
        "    theta = theta - lr * grad\n",
        "    history.append(theta)\n",
        "\n",
        "plt.plot(x, y, label=\"Loss Function y=xÂ²\")\n",
        "plt.scatter(history, [h**2 for h in history], c=\"red\")\n",
        "plt.title(\"Gradient Descent on a Simple Quadratic\")\n",
        "plt.xlabel(\"Î¸\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Tip**: decreasing learning rate too much slows convergence; too large makes it unstable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ Evaluation Metrics\n",
        "\n",
        "Different problems require different metrics.\n",
        "\n",
        "| Task | Metric | Intuition |\n",
        "|------|--------|----------|\n",
        "| **Classification** | Accuracy | % of correct predictions |\n",
        "| | Precision/Recall/F1 | trade-offs in imbalanced data |\n",
        "| | ROC-AUC | ability to rank positives over negatives |\n",
        "| **Regression** | MSE/MAE | average prediction error |\n",
        "| **Generation** | Perplexity, BLEU, ROUGE | quality of generated text |\n",
        "\n",
        "### ðŸ§  Why metrics matter:\n",
        "Optimization tells you how well you minimize loss, but evaluation tells you how useful the model is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Playground: Logistic Regression on Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load and split\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ Biasâ€“Variance Tradeoff\n",
        "\n",
        "- **Bias**: error from oversimplifying (underfitting).\n",
        "- **Variance**: error from overreacting to noise (overfitting).\n",
        "- **Goal**: find a balance â€” model complexity should match data complexity.\n",
        "\n",
        "### ðŸ§  Rule of thumb: \n",
        "More complex models (deep nets) need more data and regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Further Reading\n",
        "\n",
        "- Deep Learning (Goodfellow et al.)\n",
        "- Scikit-learn Documentation\n",
        "- CS229 Notes on Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ§© Section 1 â€“ NLP Basics\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "\n",
        "- Understand what tokens and vocabularies are and why tokenization matters.\n",
        "- Compare common tokenization algorithms (BPE, WordPiece, UnigramLM).\n",
        "- Understand embeddings: what they are, how static and contextual embeddings differ, and why contextualization matters.\n",
        "- Build a simple n-gram language model and compute perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 1.1 Tokens & Vocabularies\n",
        "\n",
        "A **token** is the smallest text unit a model processes.\n",
        "**Tokenization** converts raw text into a sequence of discrete symbols (tokens).\n",
        "\n",
        "### Example:\n",
        "\n",
        "```\n",
        "\"Transformers are changing NLP!\"\n",
        "â†’ [Transformers, are, changing, NLP, !]\n",
        "```\n",
        "\n",
        "Different tokenization strategies trade off vocabulary size vs. expressiveness:\n",
        "\n",
        "| Level | Example | Pros | Cons |\n",
        "|-------|---------|------|------|\n",
        "| **Character** | 't', 'r', 'a', ... | Handles any text | Very long sequences |\n",
        "| **Word** | 'Transformers' | Intuitive | Huge vocabularies, OOV issues |\n",
        "| **Subword** | 'transform', '##ers' | Balanced | Slightly complex preprocessing |\n",
        "\n",
        "### ðŸ§  Key Idea:\n",
        "Subword tokenization allows models to handle unseen words by composing them from smaller familiar units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Playground â€” Inspecting Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "text = \"Transformers are changing NLP!\"\n",
        "\n",
        "for model in [\"gpt2\", \"bert-base-uncased\", \"t5-small\"]:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"{model:20} â†’ {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observe** how each tokenizer splits text differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 1.2 Tokenization Algorithms\n",
        "\n",
        "### Byte-Pair Encoding (BPE) â€“ [Sennrich et al., 2016]\n",
        "Iteratively merges the most frequent pair of symbols into one unit.\n",
        "â†’ used by GPT-2, RoBERTa.\n",
        "\n",
        "### WordPiece â€“ [Wu et al., 2016]\n",
        "Similar to BPE but selects merges to maximize likelihood of training data.\n",
        "â†’ used by BERT.\n",
        "\n",
        "### Unigram Language Model â€“ [Kudo 2018]\n",
        "Starts with many candidate tokens and prunes those with low likelihood.\n",
        "â†’ used by T5 and SentencePiece.\n",
        "\n",
        "### ðŸ§  Why it matters:\n",
        "Efficient tokenization reduces OOV errors and improves compression of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 1.3 Embeddings â€” Representing Meaning as Vectors\n",
        "\n",
        "After tokenization, each token is mapped to a numerical vector called an **embedding**.\n",
        "\n",
        "### ðŸ”¸ Static Embeddings\n",
        "\n",
        "Same vector for each word, regardless of context.\n",
        "\n",
        "- **Examples**: Word2Vec, GloVe.\n",
        "- Capture semantic similarity:\n",
        "  - king â€“ man + woman â‰ˆ queen.\n",
        "\n",
        "### ðŸ”¸ Contextual Embeddings\n",
        "\n",
        "Representations depend on surrounding words.\n",
        "\n",
        "- **Examples**: ELMo, BERT.\n",
        "- The word \"bank\" gets different vectors in\n",
        "  - \"river bank\" vs. \"bank loan\".\n",
        "\n",
        "### ðŸ§  Mathematically:\n",
        "A model learns an embedding matrix $E \\in \\mathbb{R}^{|V| \\times d}$, where each row corresponds to a word vector of dimension $d$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Playground â€” Compare Word2Vec and BERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "sentence = \"The king rules the kingdom\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(\"Tokens:\", tokenizer.tokenize(sentence))\n",
        "print(\"Embedding shape:\", outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Optional Exercise**: visualize embeddings with PCA/t-SNE to see clustering patterns.\n",
        "\n",
        "### ðŸ“š References:\n",
        "\n",
        "- Word2Vec (Mikolov et al., 2013)\n",
        "- GloVe (Pennington et al., 2014)\n",
        "- ELMo (Peters et al., 2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 1.4 n-gram Language Models & Perplexity\n",
        "\n",
        "An **n-gram model** predicts the next word using the previous n â€“ 1 words.\n",
        "\n",
        "$P(w_t | w_{t-1}, \\ldots, w_{t-n+1}) \\approx \\frac{C(w_{t-n+1}^t)}{C(w_{t-n+1}^{t-1})}$\n",
        "\n",
        "where $C$ counts word occurrences.\n",
        "\n",
        "**Perplexity (PPL)** measures predictive uncertainty:\n",
        "\n",
        "$\\text{PPL} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i | context)}$\n",
        "\n",
        "Lower PPL â†’ better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Playground â€” Train a Trigram LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "# Get the corpus + tokenizer data (both 'punkt' and 'punkt_tab' for newer NLTK)\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # harmless if already present / on older NLTK\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "\n",
        "# Prepare training sentences (lowercased)\n",
        "sents = [list(map(str.lower, s)) for s in reuters.sents()[:2000]]\n",
        "\n",
        "# Build 3-gram training stream with padding\n",
        "n = 3\n",
        "train_data, vocab = padded_everygram_pipeline(n, sents)\n",
        "\n",
        "# Fit the model\n",
        "lm = MLE(n)\n",
        "lm.fit(train_data, vocab)\n",
        "\n",
        "# Evaluate perplexity on a padded test sentence\n",
        "test = ['the', 'market', 'is', 'volatile']\n",
        "padded_test = list(pad_both_ends(test, n))\n",
        "print(\"Perplexity on ['the','market','is','volatile'] â†’\", lm.perplexity(padded_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Takeaway:\n",
        "Before Transformers, such statistical LMs formed the foundation of NLP.\n",
        "They inspired the concept of predicting words from context â€” a principle that remains central in modern LLMs.\n",
        "\n",
        "### ðŸ“š Further Reading\n",
        "\n",
        "- Speech and Language Processing (Jurafsky & Martin)\n",
        "- BPE Paper\n",
        "- SentencePiece (UnigramLM)\n",
        "- Hugging Face Tokenizers Docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# âš™ï¸ Section 2 â€“ Transformers Refresher\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "\n",
        "- Recall why sequence models before Transformers (RNNs, LSTMs) struggled with long dependencies.\n",
        "- Understand the architecture of the Transformer (encoder, decoder, attention).\n",
        "- Grasp the key equations and intuition behind self-attention.\n",
        "- Recognize the role of positional encoding and multi-head attention.\n",
        "- See a mini-demo of attention in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.1 From RNNs to Transformers\n",
        "\n",
        "Before 2017, most NLP models used RNNs or LSTMs.\n",
        "They processed sequences step-by-step, making them slow and prone to losing long-range information.\n",
        "\n",
        "| Model | Limitation |\n",
        "|-------|------------|\n",
        "| **RNN** | Vanishing gradients, sequential computation |\n",
        "| **LSTM/GRU** | Better memory but still sequential |\n",
        "| **Transformer** | Fully parallel, learns long-range dependencies via attention |\n",
        "\n",
        "### ðŸ§  Key Idea:\n",
        "Instead of passing information through time steps, Transformers let every token attend to every other token directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.2 The Transformer Architecture (Vaswani et al., 2017)\n",
        "\n",
        "**Encoderâ€“Decoder structure**:\n",
        "\n",
        "- **Encoder**: converts input sequence â†’ contextual representations.\n",
        "- **Decoder**: generates output sequence using encoder context + previously generated tokens.\n",
        "\n",
        "Each layer contains:\n",
        "\n",
        "- Multi-Head Self-Attention\n",
        "- Feed-Forward Network\n",
        "- Layer Norm + Residual Connections\n",
        "\n",
        "### ðŸ§© Simplified Diagram\n",
        "\n",
        "```\n",
        "Input Embeddings â†’ [Self-Attention â†’ FeedForward]Ã—N â†’ Encoder Output\n",
        "Decoder Input â†’ [Masked-Self-Attention + Cross-Attention + FeedForward]Ã—N â†’ Output logits\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.3 Self-Attention: Core Mechanism\n",
        "\n",
        "Given token embeddings $x_i \\in \\mathbb{R}^d$, we compute three projections:\n",
        "\n",
        "$Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V$\n",
        "\n",
        "Then each token attends to others:\n",
        "\n",
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$\n",
        "\n",
        "### ðŸ§  Intuition:\n",
        "\n",
        "- **Q (Query)** asks: \"What am I looking for?\"\n",
        "- **K (Key)** answers: \"What do I contain?\"\n",
        "- **V (Value)** provides the actual information.\n",
        "\n",
        "The dot-product similarity decides how much one token cares about another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Playground â€” Tiny Attention Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 3 tokens, embedding dim 4\n",
        "x = torch.randn(3, 4)\n",
        "W_Q = torch.randn(4, 4)\n",
        "W_K = torch.randn(4, 4)\n",
        "W_V = torch.randn(4, 4)\n",
        "\n",
        "Q, K, V = x @ W_Q, x @ W_K, x @ W_V\n",
        "attn_scores = Q @ K.T / (4 ** 0.5)\n",
        "attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "output = attn_weights @ V\n",
        "\n",
        "print(\"Attention weights:\\n\", attn_weights)\n",
        "print(\"Output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Tip**: each row of attn_weights shows how strongly a token attends to the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.4 Multi-Head Attention\n",
        "\n",
        "Instead of one big attention map, Transformers use multiple heads that learn complementary relationships.\n",
        "\n",
        "$\\text{MHA}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$\n",
        "\n",
        "### ðŸ§  Analogy: \n",
        "Multiple people reading a paragraph and focusing on different detailsâ€”syntax, topic, entities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.5 Positional Encoding\n",
        "\n",
        "Because attention has no inherent order awareness, we inject positional encodings:\n",
        "\n",
        "$PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$\n",
        "\n",
        "### ðŸ§  Intuition: \n",
        "Adds a smooth notion of word order; adjacent positions have similar encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def positional_encoding(seq_len=50, d=16):\n",
        "    pos = np.arange(seq_len)[:, None]\n",
        "    i = np.arange(d)[None, :]\n",
        "    angles = pos / np.power(10000, (2*(i//2))/d)\n",
        "    pe = np.zeros((seq_len, d))\n",
        "    pe[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    pe[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    return pe\n",
        "\n",
        "plt.imshow(positional_encoding(), aspect='auto', cmap='viridis')\n",
        "plt.title(\"Positional Encoding Patterns\")\n",
        "plt.xlabel(\"Embedding Dimension\")\n",
        "plt.ylabel(\"Token Position\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.6 Feed-Forward & Normalization Layers\n",
        "\n",
        "Each attention block is followed by:\n",
        "\n",
        "**A position-wise Feed-Forward Network**:\n",
        "\n",
        "$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$\n",
        "\n",
        "**Residual connection + LayerNorm**\n",
        "\n",
        "These help with stability and gradient flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.7 Training Objective (Language Modeling)\n",
        "\n",
        "Two major pretraining styles emerged:\n",
        "\n",
        "| Model Type | Objective | Example |\n",
        "|------------|-----------|----------|\n",
        "| **Autoregressive** | Predict next token given past | GPT-series |\n",
        "| **Masked LM** | Predict masked tokens | BERT |\n",
        "| **Denoising Seq-to-Seq** | Reconstruct corrupted text | T5 |\n",
        "\n",
        "We'll dive deeper into these in Section 3 â€“ Language Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 2.8 Why \"Attention Is All You Need\" Matters\n",
        "\n",
        "- Enables full parallelization (no recurrence).\n",
        "- Learns global dependencies efficiently.\n",
        "- Scales to billions of parameters and diverse modalities (text, vision, speech).\n",
        "\n",
        "### ðŸ§  Key Takeaway:\n",
        "Transformers are a general-purpose architecture â€” not just for text but for anything that can be represented as a sequence of tokens or patches.\n",
        "\n",
        "### ðŸ“š Further Reading\n",
        "\n",
        "- Vaswani et al., 2017 â€“ Attention Is All You Need\n",
        "- Illustrated Transformer â€“ Jay Alammar\n",
        "- Hugging Face Transformers Docs\n",
        "- Annotated Transformer (Pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ—£ï¸ Section 3 â€“ Language Models\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "\n",
        "- Understand what a language model is and how it assigns probabilities to sequences of text.\n",
        "- Differentiate between small and large LMs, and how scaling affects their behavior.\n",
        "- Grasp common pretraining objectives: Autoregressive (GPT), Masked (BERT), and Denoising (T5).\n",
        "- Learn about data pipelines for pretraining and fine-tuning.\n",
        "- Experiment with generating text and measuring perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 3.1 What Is a Language Model?\n",
        "\n",
        "A **Language Model (LM)** estimates the probability of a sequence of words (or tokens):\n",
        "\n",
        "$P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^{T} P(w_t | w_{<t})$\n",
        "\n",
        "**Goal**: assign higher probabilities to valid sentences (\"The cat sat on the mat\")\n",
        "and lower to unlikely ones (\"Mat sat cat the on\").\n",
        "\n",
        "### ðŸ§  Intuition:\n",
        "Language modeling teaches machines the structure, syntax, and semantics of a language â€” it's how LMs \"understand\" text statistically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 3.2 Small vs. Large Language Models\n",
        "\n",
        "| Category | Example | Typical Size | Usage |\n",
        "|----------|---------|--------------|-------|\n",
        "| **Small** | DistilBERT, ALBERT, GPT-2-small | < 200M parameters | Task-specific fine-tuning |\n",
        "| **Medium** | BERT-large, GPT-2-xl | 300Mâ€“1B | Good for academic tasks, lower compute |\n",
        "| **Large** | GPT-3, PaLM, LLaMA 3, Claude 3 | 10Bâ€“>1T | General-purpose reasoning, few-shot learning |\n",
        "\n",
        "### ðŸ§  Scaling Law Insight (Kaplan et al., 2020):\n",
        "Performance improves predictably with model size, data, and compute, following power laws â€” until you run out of high-quality data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 3.3 Pretraining Objectives\n",
        "\n",
        "### ðŸ”¸ (a) Autoregressive Language Modeling (GPT)\n",
        "\n",
        "Predict the next token given all previous ones:\n",
        "\n",
        "$L_{AR} = -\\sum_t \\log P(w_t | w_{<t})$\n",
        "\n",
        "**Used by**: GPT, LLaMA, Mistral\n",
        "\n",
        "ðŸ§  **Generative direction**: left â†’ right\n",
        "\n",
        "ðŸ’¬ **Example**: Given \"The sky is\", predict \"blue\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ”¸ (b) Masked Language Modeling (MLM â€“ BERT)\n",
        "\n",
        "Randomly mask ~15% of tokens, and train model to predict them:\n",
        "\n",
        "$L_{MLM} = -\\sum_{i \\in M} \\log P(w_i | w_{\\setminus M})$\n",
        "\n",
        "**Used by**: BERT, RoBERTa, DeBERTa\n",
        "\n",
        "ðŸ§  **Bidirectional context**: both left and right sides are visible.\n",
        "\n",
        "ðŸ’¬ **Example**: \"The [MASK] is blue\" â†’ predict \"sky\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ”¸ (c) Denoising Autoencoding (T5-style)\n",
        "\n",
        "Corrupt spans of text (not just words) and train to reconstruct them:\n",
        "\n",
        "```\n",
        "Input: \"The sky is <mask_0>\"     Target: \"blue\"\n",
        "```\n",
        "\n",
        "**Used by**: T5, BART\n",
        "\n",
        "ðŸ§  **Seq-to-seq framing** allows flexible pretraining for multiple downstream tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 3.4 Data Pipelines: Collection â†’ Cleaning â†’ Tokenization\n",
        "\n",
        "Modern LLM training requires massive, high-quality text corpora.\n",
        "\n",
        "### Typical Pipeline:\n",
        "\n",
        "1. **Collection**\n",
        "   - Web crawl (Common Crawl, Wikipedia, books, code)\n",
        "   - Domain-specific datasets (arXiv, StackExchange, etc.)\n",
        "\n",
        "2. **Cleaning & Filtering**\n",
        "   - Remove duplicates, profanity, spam, boilerplate.\n",
        "   - Filter by language, readability, and toxicity (using heuristics or classifiers).\n",
        "\n",
        "3. **Tokenization**\n",
        "   - Apply BPE/WordPiece/Unigram.\n",
        "   - Produce compact integer sequences for model input.\n",
        "\n",
        "4. **Sharding & Streaming**\n",
        "   - Split into ~1GB shards for distributed training.\n",
        "   - Stream efficiently using frameworks like ðŸ¤— Datasets.\n",
        "\n",
        "### ðŸ§  Practical Tip:\n",
        "The quality of data affects LLM performance more than sheer quantity after a certain scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 3.5 Playground â€” Text Generation Demo (Autoregressive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's generate text using a pretrained autoregressive LM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Artificial intelligence is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(**inputs, max_length=40, temperature=0.8, top_p=0.9, do_sample=True)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Try adjusting**:\n",
        "- `temperature`: randomness of sampling\n",
        "- `top_p`: nucleus sampling cutoff\n",
        "- `max_length`: generation length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 3.6 Playground â€” Perplexity Evaluation\n",
        "\n",
        "Perplexity measures how well a model predicts a sequence (lower is better):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:1%]\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
        "\n",
        "max_length = model.config.n_positions\n",
        "stride = 512\n",
        "nlls = []\n",
        "\n",
        "for i in range(0, encodings.input_ids.size(1), stride):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-stride] = -100  # mask out context tokens\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        nll = outputs.loss * stride\n",
        "    nlls.append(nll)\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
        "print(f\"Perplexity: {ppl.item():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 3.7 Practical Uses of Language Models\n",
        "\n",
        "- Autocompletion (e.g., code, text)\n",
        "- Summarization, translation, question answering\n",
        "- Information retrieval and RAG (retrieval-augmented generation)\n",
        "- Conversational agents (like ChatGPT!)\n",
        "- Few-shot reasoning and tool use\n",
        "\n",
        "### ðŸ§  Key Insight:\n",
        "A pretrained LM captures broad language knowledge.\n",
        "Fine-tuning or prompting adapts it to specific tasks â€” this is the bridge from foundation model â†’ application model.\n",
        "\n",
        "### ðŸ“š Further Reading\n",
        "\n",
        "- GPT Paper â€“ Radford et al. (2018)\n",
        "- BERT Paper â€“ Devlin et al. (2019)\n",
        "- T5 Paper â€“ Raffel et al. (2020)\n",
        "- The Scaling Laws for Neural Language Models (Kaplan et al., 2020)\n",
        "- Hugging Face Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ§© Section 4 â€“ Fine-Tuning and Parameter-Efficient Adaptation\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "\n",
        "- Understand the difference between full fine-tuning and parameter-efficient tuning.\n",
        "- Learn the concepts of prompt-tuning, prefix-tuning, and LoRA/adapters.\n",
        "- See small code demos of fine-tuning a model for classification tasks.\n",
        "- Understand when to use each method for efficiency, memory, and compute constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 4.1 Full Fine-Tuning\n",
        "\n",
        "Full fine-tuning updates **all parameters** of a pretrained LM for a downstream task.\n",
        "\n",
        "**Pros**: Maximum flexibility; usually gives best performance.\n",
        "\n",
        "**Cons**: Very memory-intensive for large LMs; retraining each time for new tasks.\n",
        "\n",
        "ðŸ’¡ **Example Use Case**: Fine-tuning GPT-2 or BERT for sentiment analysis on IMDB dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load small subset for demo\n",
        "dataset = load_dataset(\"imdb\", split=\"train[:1%]\").train_test_split(test_size=0.2)\n",
        "\n",
        "# Model & tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Preprocess\n",
        "def preprocess(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "encoded = dataset.map(preprocess, batched=True)\n",
        "\n",
        "# Trainer setup\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=5,\n",
        ")\n",
        "trainer = Trainer(model=model, args=args, train_dataset=encoded[\"train\"], eval_dataset=encoded[\"test\"])\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Observation**: Full fine-tuning works well for smaller models, but is impractical for billion-parameter LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 4.2 Prompt-Tuning\n",
        "\n",
        "Prompt-tuning **freezes** the model parameters and learns small task-specific embeddings (the \"soft prompt\") prepended to the input.\n",
        "\n",
        "**Pros**: Extremely memory efficient; easy to swap prompts for new tasks.\n",
        "\n",
        "**Cons**: Slightly lower performance than full fine-tuning.\n",
        "\n",
        "### ðŸ§  Analogy: \n",
        "Think of giving the model a special hint before it reads your text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 4.3 Prefix-Tuning\n",
        "\n",
        "Similar to prompt-tuning, but adds learnable vectors to **every layer** of the Transformer (prefix).\n",
        "\n",
        "- Works better than plain prompt-tuning for large models.\n",
        "- Still parameter-efficient, training only a fraction of total weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 4.4 LoRA (Low-Rank Adaptation)\n",
        "\n",
        "LoRA injects low-rank matrices into pretrained weights:\n",
        "\n",
        "$W' = W + \\Delta W, \\quad \\Delta W = AB, \\quad \\text{rank}(A), \\text{rank}(B) \\ll d$\n",
        "\n",
        "**Pros**: Fine-tune large models with minimal GPU memory.\n",
        "\n",
        "**Cons**: Requires compatible frameworks (Hugging Face PEFT, DeepSpeed).\n",
        "\n",
        "ðŸ’¡ **Key idea**: You don't touch the main weights; you just learn small \"delta\" matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 4.5 Adapters\n",
        "\n",
        "Add small feed-forward networks inside each layer of the Transformer.\n",
        "\n",
        "- Only adapter weights are trained; the main LM remains frozen.\n",
        "- Very similar in concept to LoRA but often layer-specific."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 4.6 Choosing the Right Method\n",
        "\n",
        "| Method | Memory | Flexibility | When to Use |\n",
        "|--------|--------|-------------|-------------|\n",
        "| **Full Fine-Tuning** | High | High | Small-medium models, max accuracy |\n",
        "| **Prompt-Tuning** | Very Low | Medium | Many tasks, low compute |\n",
        "| **Prefix-Tuning** | Low | Medium | Medium-large models |\n",
        "| **LoRA/Adapters** | Low | High | Large models, multi-task or frequent updates |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§© Playground â€“ LoRA Fine-Tuning with PEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Load model\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=4, lora_alpha=16, target_modules=[\"c_attn\"], lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Dummy data\n",
        "texts = [\"Hello world\", \"Machine learning is fun\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
        "labels = inputs.input_ids.clone()\n",
        "\n",
        "# Simple forward pass\n",
        "outputs = model(**inputs, labels=labels)\n",
        "print(\"Loss:\", outputs.loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Observation**: LoRA allows us to fine-tune GPT-2 small with just a fraction of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 4.7 Key Takeaways\n",
        "\n",
        "- **Full fine-tuning**: flexible but expensive.\n",
        "- **Prompt/Prefix tuning**: cheap, good for multi-task pipelines.\n",
        "- **LoRA/Adapters**: optimal for large LMs; you train small delta matrices.\n",
        "- Efficient methods enable multi-task use and frequent updates without retraining the full model.\n",
        "\n",
        "### ðŸ“š References\n",
        "\n",
        "- LoRA Paper â€“ Hu et al., 2021\n",
        "- Prefix-Tuning â€“ Li & Liang, 2021\n",
        "- Prompt-Tuning â€“ Lester et al., 2021\n",
        "- Hugging Face PEFT Docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ§© Section 5 â€“ Inference Strategies and RAG\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "\n",
        "- Understand how LLMs generate text and the key parameters affecting output.\n",
        "- Learn sampling strategies: greedy, beam search, temperature, top-k, top-p (nucleus).\n",
        "- Explore context windows and memory augmentation.\n",
        "- Understand retrieval-augmented generation (RAG) and its practical usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 5.1 Generating Text with LLMs\n",
        "\n",
        "A pretrained LM outputs logits over the vocabulary for the next token.\n",
        "Generation involves selecting a token at each step and feeding it back as input.\n",
        "\n",
        "### Key concepts:\n",
        "\n",
        "- **Greedy decoding**: pick token with max probability. Simple but deterministic.\n",
        "- **Sampling**: randomly pick according to probabilities. Adds creativity.\n",
        "- **Beam search**: maintain multiple top sequences to balance probability and diversity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 5.2 Sampling Strategies\n",
        "\n",
        "| Strategy | How It Works | Pros | Cons |\n",
        "|----------|--------------|------|------|\n",
        "| **Greedy** | argmax token each step | Fast, deterministic | Can be repetitive, low diversity |\n",
        "| **Beam Search** | Keep top-k sequences at each step | Higher quality | Slower, may overfit to high-prob sequences |\n",
        "| **Temperature** | Scale logits by 1/T before softmax | Controls randomness | T too high â†’ gibberish |\n",
        "| **Top-k** | Only sample from top k tokens | Reduces low-prob tokens | k too low â†’ repetitive |\n",
        "| **Top-p (nucleus)** | Sample from smallest set with cumulative prob â‰¥ p | Dynamic, better diversity | Slightly slower |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 5.3 Playground â€” Text Generation with Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"In the future, AI will\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Greedy decoding\n",
        "greedy_output = model.generate(**inputs, max_length=40)\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-p sampling\n",
        "top_p_output = model.generate(**inputs, max_length=40, do_sample=True, top_p=0.9, temperature=0.8)\n",
        "print(\"Top-p Sampling:\", tokenizer.decode(top_p_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Tip**: Experiment with temperature, top-k, and top-p to see how randomness affects creativity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 5.4 Context Window & Memory\n",
        "\n",
        "- **Context window**: maximum number of tokens a model can see at once.\n",
        "- Longer context allows richer understanding, but increases memory usage.\n",
        "- For very long documents, models may truncate input or use sliding windows.\n",
        "- **Memory & Retrieval Augmentation**: LLMs can query external knowledge to extend context beyond their native window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 5.5 Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "**Idea**: Combine LLM with an external knowledge base for better factual accuracy.\n",
        "\n",
        "### Pipeline:\n",
        "\n",
        "1. Query embedding â†’ search in vector database (FAISS, Milvus, Pinecone).\n",
        "2. Retrieve top-k relevant documents.\n",
        "3. Concatenate with prompt â†’ feed to LLM.\n",
        "4. Generate answer using both model knowledge + retrieved context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 5.6 Playground â€” RAG with Hugging Face + FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Dummy documents\n",
        "docs = [\"AI can generate text.\", \"Transformers replaced RNNs.\", \"BERT uses masked language modeling.\"]\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_texts(docs, embeddings)\n",
        "\n",
        "query = \"How does BERT work?\"\n",
        "retrieved = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "# Show retrieved context\n",
        "print(\"Retrieved:\", [d.page_content for d in retrieved])\n",
        "\n",
        "# Use retrieved context for generation\n",
        "prompt = query + \" Context: \" + \" \".join([d.page_content for d in retrieved])\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "print(\"Generated Answer:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Observation**: RAG improves factual grounding without retraining the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 5.7 Key Takeaways\n",
        "\n",
        "- LLM outputs are probabilistic, and generation quality depends on decoding strategy.\n",
        "- Top-p / temperature sampling gives more diverse outputs than greedy or beam search.\n",
        "- RAG augments context beyond the model's native memory, improving factuality.\n",
        "- Vector databases + embeddings make retrieval fast and scalable.\n",
        "\n",
        "### ðŸ“š References\n",
        "\n",
        "- LangChain Docs\n",
        "- FAISS â€“ Facebook AI Similarity Search\n",
        "- Hugging Face Retrieval + RAG\n",
        "- Top-p / Top-k sampling explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ§© Section 6 â€“ Scaling and Efficiency Techniques\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "\n",
        "- Understand the tradeoffs between model size, compute, and performance.\n",
        "- Learn practical strategies for memory-efficient training and inference.\n",
        "- Be aware of distillation, quantization, and parameter-efficient tuning techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 6.1 Model Size Tradeoffs\n",
        "\n",
        "| Aspect | Small Model | Large Model |\n",
        "|--------|-------------|-------------|\n",
        "| **Parameters** | Few million â†’ <200M | Billions â†’ >1T |\n",
        "| **FLOPs** | Low | Very high |\n",
        "| **Training Data** | Modest | Massive |\n",
        "| **Generalization** | Task-specific | Broad, few-shot/flexible |\n",
        "| **Inference Cost** | Cheap | Expensive (GPU/TPU) |\n",
        "\n",
        "ðŸ’¡ **Rule of Thumb**: Bigger models perform better but require careful compute/memory planning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 6.2 FLOPs, Batching & Mixed Precision\n",
        "\n",
        "- **FLOPs (Floating Point Operations)**: total compute per forward/backward pass.\n",
        "- **Batching**: processing multiple sequences together amortizes compute; larger batch â†’ faster training (but more memory).\n",
        "- **Mixed Precision (FP16 / bfloat16)**: halves memory usage, doubles throughput, minimal loss in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Enabling mixed precision in PyTorch Trainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    fp16=True,  # mixed precision\n",
        "    num_train_epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 6.3 Memory Optimization\n",
        "\n",
        "| Technique | Description |\n",
        "|-----------|-------------|\n",
        "| **Gradient Checkpointing** | Save memory by recomputing activations during backward pass |\n",
        "| **Gradient Accumulation** | Simulate large batch size by accumulating gradients over multiple steps |\n",
        "| **Activation Offloading** | Move intermediate activations to CPU or disk |\n",
        "\n",
        "ðŸ’¡ These techniques make training large models feasible on limited GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 6.4 Distillation & Quantization\n",
        "\n",
        "- **Distillation**: Train a smaller \"student\" model to mimic a larger \"teacher\" â†’ faster, lighter, still high-quality.\n",
        "- **Quantization**: Reduce weight precision (e.g., FP32 â†’ INT8/INT4) â†’ smaller memory footprint, faster inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 6.5 Parameter-Efficient Tuning Recap\n",
        "\n",
        "- LoRA, adapters, prompt/prefix tuning allow large LMs to adapt without full fine-tuning.\n",
        "- Reduces memory usage, training time, and storage for multiple tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 6.6 Key Takeaways\n",
        "\n",
        "- Scaling up increases performance but requires FLOPs, memory, and infrastructure planning.\n",
        "- Efficiency hacks (mixed precision, checkpointing, accumulation) make large-scale training practical.\n",
        "- Distillation/quantization and parameter-efficient tuning allow deployment on edge devices or multiple tasks.\n",
        "\n",
        "### ðŸ“š References\n",
        "\n",
        "- Hugging Face Accelerate\n",
        "- Mixed Precision in PyTorch\n",
        "- DistilBERT Paper\n",
        "- LoRA Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ§© Section 7 â€“ Evaluation & Metrics\n",
        "\n",
        "## ðŸ§­ Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "\n",
        "- Understand intrinsic vs extrinsic evaluation metrics for LLMs.\n",
        "- Learn how perplexity, token accuracy, BLEU/ROUGE, F1, and AUROC work.\n",
        "- Gain insight into calibration and hallucination detection.\n",
        "- Explore explainability tools like attention probes and saliency maps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 7.1 Intrinsic vs Extrinsic Metrics\n",
        "\n",
        "| Type | Measures | Use Case |\n",
        "|------|----------|----------|\n",
        "| **Intrinsic** | Model's internal predictive quality | Language modeling, token prediction |\n",
        "| **Extrinsic** | Downstream task performance | QA, summarization, translation |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 7.2 Intrinsic Metrics\n",
        "\n",
        "### Perplexity (PPL):\n",
        "\n",
        "$\\text{PPL} = 2^{-\\frac{1}{N}\\sum_{i=1}^{N} \\log_2 P(w_i|\\text{context})}$\n",
        "\n",
        "Lower is better; measures how well the LM predicts the next token.\n",
        "\n",
        "### Token Accuracy: \n",
        "Proportion of correctly predicted tokens (mostly for masked or autoregressive LM evaluation).\n",
        "\n",
        "ðŸ’¡ **Intuition**: Intrinsic metrics measure language modeling skill, independent of any specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 7.3 Extrinsic Metrics\n",
        "\n",
        "| Task | Metric | Description |\n",
        "|------|--------|-------------|\n",
        "| **Translation / Summarization** | BLEU, ROUGE | Compare generated text vs reference |\n",
        "| **Classification** | F1, Accuracy | Task-specific correctness |\n",
        "| **Retrieval / QA** | Exact Match, F1 | Match predicted answer to ground truth |\n",
        "| **General** | AUROC | For probabilistic classification, measures ranking quality |\n",
        "\n",
        "ðŸ’¡ **Tip**: Always pair intrinsic + extrinsic metrics for full evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 7.4 Calibration & Hallucination\n",
        "\n",
        "- **Calibration**: Do predicted probabilities reflect true likelihood?\n",
        "- **Hallucination**: When LLM generates plausible but incorrect information.\n",
        "- **Techniques to mitigate**:\n",
        "  - Prompting with factual context (RAG)\n",
        "  - Post-hoc verification\n",
        "  - Using uncertainty scores or multiple hypotheses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 7.5 Explainability Tools\n",
        "\n",
        "| Method | Description |\n",
        "|--------|-------------|\n",
        "| **Attention Probes** | Visualize attention weights to see which tokens influence output |\n",
        "| **Saliency Maps / Gradients** | Measure how input tokens affect predictions |\n",
        "| **Integrated Gradients** | Attribute prediction score to input features |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Visualizing attention weights from BERT\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "\n",
        "text = \"Transformers are changing NLP\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "attentions = outputs.attentions  # tuple of attention matrices\n",
        "\n",
        "# Visualize first layer, first head\n",
        "plt.matshow(attentions[0][0][0].numpy())\n",
        "plt.title(\"Attention weights: Layer 1, Head 1\")\n",
        "plt.xlabel(\"Token Position\")\n",
        "plt.ylabel(\"Token Position\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’¡ **Observation**: You can see which words \"attend\" to which, helping interpret model behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¹ 7.6 Key Takeaways\n",
        "\n",
        "- Intrinsic metrics evaluate language modeling; extrinsic metrics evaluate downstream task performance.\n",
        "- Calibration and hallucination detection are crucial for safe LLM deployment.\n",
        "- Explainability helps debug and understand model decisions, building trust.\n",
        "\n",
        "### ðŸ“š References\n",
        "\n",
        "- Perplexity & Language Modeling\n",
        "- BLEU Paper\n",
        "- ROUGE Paper\n",
        "- Interpretability in Transformers â€“ Vig, 2019\n",
        "- LLM Hallucination Analysis â€“ Ji et al., 2023"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
